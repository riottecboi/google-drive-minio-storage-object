{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Should install java at the machine before using Spark\n",
    "sudo apt update\n",
    "# Execute the following command to install the JRE from OpenJDK 11:\n",
    "sudo apt install default-jre\n",
    "# The JRE will allow you to run almost all Java software.\n",
    "# Verify the installation with:\n",
    "java -version\n",
    "# You may need the JDK in addition to the JRE in order to compile and run some specific Java-based software. To install the JDK, execute the following command, which will also install the JRE:\n",
    "sudo apt install default-jdk\n",
    "# Verify that the JDK is installed by checking the version of javac, the Java compiler:\n",
    "javac -version"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad14a1fbbb608156"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class FILE_TRANSFORM():\n",
    "    def __init__(self):\n",
    "        self.minio_bucket = \"data\"\n",
    "        self.minio_object = \"data-result\"\n",
    "        self.obj_storage_access_key = os.getenv('OBJ_STORAGE_ACCESS_KEY', 'access-key')\n",
    "        self.obj_storage_secret_key = os.getenv('OBJ_STORAGE_SECRET_KEY', 'secret-key')\n",
    "        self.obj_storage_endpoint = os.getenv('OBJ_STORAGE_ENDPOINT', 'http://localhost:9000')\n",
    "        self.paths = [\"s3a://data/data-raw/data.json\",\n",
    "                     \"s3a://data/data-raw/data2.json\",\n",
    "                     \"s3a://data/data-raw/data3.json\"]\n",
    "\n",
    "    def transform_file(self):\n",
    "        # REFERENCE FROM THIS THREAD https://stackoverflow.com/a/77592717 \n",
    "        builder = SparkSession.builder.appName(\"Python Spark SQL Sample Transformer\") \\\n",
    "                    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.access.key\", self.obj_storage_access_key) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.secret.key\", self.obj_storage_secret_key) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.endpoint\", self.obj_storage_endpoint) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "                    .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "\n",
    "        # spark.hadoop.fs.s3a.path.style.access\n",
    "        # see https://stackoverflow.com/questions/61552054/spark-path-style-access-with-fs-s3a-path-style-access-property-is-not-working\n",
    "        # see https://github.com/minio/minio/issues/7020\n",
    "\n",
    "        # https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws\n",
    "        # and also https://github.com/delta-io/delta/issues/895\n",
    "        # and also https://stackoverflow.com/questions/44411493/java-lang-noclassdeffounderror-org-apache-hadoop-fs-storagestatistics\n",
    "        # and also https://stackoverflow.com/questions/76858933/noclassdeffounderror-raised-when-reading-minio-data-using-pyspark\n",
    "        my_packages = [\"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "                       \"org.apache.hadoop:hadoop-client-runtime:3.3.4\",\n",
    "                       \"org.apache.hadoop:hadoop-client-api:3.3.4\",\n",
    "                       \"io.delta:delta-contribs_2.12:3.0.0\",\n",
    "                       \"io.delta:delta-hive_2.12:3.0.0\",\n",
    "                       \"com.amazonaws:aws-java-sdk-bundle:1.12.603\",\n",
    "                       ]\n",
    "\n",
    "        # Create a Spark instance with the builder\n",
    "        # As a result, you now can read and write Delta tables\n",
    "        spark = configure_spark_with_delta_pip(builder, extra_packages=my_packages).getOrCreate()\n",
    "\n",
    "        # Read data from each path and union them\n",
    "        dfs = []\n",
    "        for path in self.paths:\n",
    "            df = spark.read.option(\"multiline\",\"true\").json(path)\n",
    "            dfs.append(df)\n",
    "\n",
    "        # Union the DataFrames\n",
    "        combined_df = dfs[0]\n",
    "        for df in dfs[1:]:\n",
    "            combined_df = combined_df.union(df).dropDuplicates()\n",
    "\n",
    "        # Show a DataFrames\n",
    "        combined_df.show()\n",
    "\n",
    "        # Write the combined DataFrame to a bucket but the result would be a folder instead of single file\n",
    "        # combined_df.write.mode('overwrite').json(\"s3a://data/data-result/result.json\")\n",
    "        # Write the combined DataFrame to a single file and pushing by upload function\n",
    "        combined_df.toPandas().to_json('result.json', orient='records', force_ascii=False, lines=True)\n",
    "        self.upload_file_to_minio('result.json', self.minio_bucket, f\"{self.minio_object}/result.json\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b48c7f4dab15eefd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "    def upload_file_to_minio(self, file_path, minio_bucket, minio_object_name):\n",
    "        s3c = boto3.resource('s3',\n",
    "                             endpoint_url=self.obj_storage_endpoint,\n",
    "                             aws_access_key_id=self.obj_storage_access_key,\n",
    "                             aws_secret_access_key=self.obj_storage_secret_key,\n",
    "                             config=boto3.session.Config(signature_version='s3v4'),\n",
    "                             verify=False\n",
    "                             )\n",
    "        s3c.Bucket(minio_bucket).upload_file(file_path, minio_object_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d47286d672810cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if __name__ == '__main__':#\n",
    "    file_transform = FILE_TRANSFORM()\n",
    "    downloading = file_transform.transform_file()\n",
    "    print('Finished')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c03ee43a775c7cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
